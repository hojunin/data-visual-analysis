import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, adjusted_rand_score, calinski_harabasz_score
from sklearn.neighbors import NearestNeighbors
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.append('..')
from utils import add_chart_export_section

def find_optimal_clusters(X, max_k=10, method='kmeans'):
    """ìµœì ì˜ í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ì°¾ê¸° ìœ„í•œ ë¶„ì„"""
    if method == 'kmeans':
        inertias = []
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(X)))
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42)
            cluster_labels = kmeans.fit_predict(X)
            
            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(X, cluster_labels))
        
        return k_range, inertias, silhouette_scores
    
    return None, None, None

def perform_pca_analysis(X, n_components=2):
    """PCA ë¶„ì„ ìˆ˜í–‰"""
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)
    
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)
    
    return X_pca, explained_variance, cumulative_variance, pca

def perform_tsne_analysis(X, n_components=2, perplexity=30):
    """t-SNE ë¶„ì„ ìˆ˜í–‰"""
    # ë°ì´í„°ê°€ ë„ˆë¬´ í¬ë©´ ìƒ˜í”Œë§
    if len(X) > 1000:
        sample_idx = np.random.choice(len(X), 1000, replace=False)
        X_sample = X[sample_idx]
    else:
        X_sample = X
        sample_idx = np.arange(len(X))
    
    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)
    X_tsne = tsne.fit_transform(X_sample)
    
    return X_tsne, sample_idx

def plot_cluster_results(X_reduced, labels, title, feature_names=None, method_name=""):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    if X_reduced.shape[1] == 2:
        fig = px.scatter(
            x=X_reduced[:, 0], y=X_reduced[:, 1],
            color=labels.astype(str),
            title=title,
            labels={'x': f'{method_name} Component 1', 'y': f'{method_name} Component 2'},
            color_discrete_sequence=px.colors.qualitative.Set3
        )
    else:
        # 3D í”Œë¡¯
        fig = px.scatter_3d(
            x=X_reduced[:, 0], y=X_reduced[:, 1], z=X_reduced[:, 2],
            color=labels.astype(str),
            title=title,
            labels={'x': f'{method_name} Component 1', 'y': f'{method_name} Component 2', 'z': f'{method_name} Component 3'},
            color_discrete_sequence=px.colors.qualitative.Set3
        )
    
    fig.update_layout(height=600)
    return fig

def main():
    st.title("ğŸ§© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    st.markdown("ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ìœ ì‚¬í•œ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì„ ê·¸ë£¹í™”í•´ë³´ì„¸ìš”.")
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv'])
    
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìì‹ ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•´ë³´ì„¸ìš”.")
        df = pd.read_csv('test.csv')
    
    st.sidebar.markdown("---")
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    if st.sidebar.checkbox("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
        st.subheader("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df.head())
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í–‰ ìˆ˜", len(df))
        with col2:
            st.metric("ì—´ ìˆ˜", len(df.columns))
        with col3:
            st.metric("ê²°ì¸¡ê°’", df.isnull().sum().sum())
    
    # í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •
    st.sidebar.header("ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì •")
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_columns) < 2:
        st.error("ìµœì†Œ 2ê°œ ì´ìƒì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # í”¼ì²˜ ì„ íƒ
    selected_features = st.sidebar.multiselect(
        "í´ëŸ¬ìŠ¤í„°ë§ì— ì‚¬ìš©í•  í”¼ì²˜ ì„ íƒ",
        numeric_columns,
        default=numeric_columns[:min(5, len(numeric_columns))]
    )
    
    if len(selected_features) < 2:
        st.warning("ìµœì†Œ 2ê°œ ì´ìƒì˜ í”¼ì²˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    X = df[selected_features].dropna()
    
    if len(X) == 0:
        st.error("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°ì¸¡ê°’ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì„ íƒ
    scaling_method = st.sidebar.selectbox(
        "ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë°©ë²•",
        ["StandardScaler", "MinMaxScaler", "ìŠ¤ì¼€ì¼ë§ ì—†ìŒ"]
    )
    
    if scaling_method == "StandardScaler":
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
    elif scaling_method == "MinMaxScaler":
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
    else:
        X_scaled = X.values
    
    # ì°¨ì› ì¶•ì†Œ ë°©ë²• ì„ íƒ
    st.sidebar.header("ğŸ“ ì°¨ì› ì¶•ì†Œ")
    dim_reduction = st.sidebar.selectbox(
        "ì°¨ì› ì¶•ì†Œ ë°©ë²•",
        ["ì—†ìŒ", "PCA", "t-SNE"]
    )
    
    n_components = st.sidebar.slider(
        "ì¶•ì†Œí•  ì°¨ì› ìˆ˜",
        2, min(10, len(selected_features)),
        2 if dim_reduction != "ì—†ìŒ" else len(selected_features),
        key="clustering_n_components"
    )
    
    # ì°¨ì› ì¶•ì†Œ ìˆ˜í–‰
    if dim_reduction == "PCA":
        X_reduced, explained_var, cumulative_var, pca_model = perform_pca_analysis(X_scaled, n_components)
        reduction_method_name = "PCA"
    elif dim_reduction == "t-SNE":
        perplexity = st.sidebar.slider("t-SNE Perplexity", 5, 50, 30, key="clustering_perplexity")
        X_reduced, sample_idx = perform_tsne_analysis(X_scaled, n_components, perplexity)
        reduction_method_name = "t-SNE"
        # t-SNEì˜ ê²½ìš° ì›ë³¸ ë°ì´í„°ë„ ìƒ˜í”Œë§ëœ ê²ƒì„ ì‚¬ìš©
        X_scaled = X_scaled[sample_idx]
        X = X.iloc[sample_idx]
    else:
        X_reduced = X_scaled
        reduction_method_name = "Original"
    
    # ì°¨ì› ì¶•ì†Œ ê²°ê³¼ ì‹œê°í™”
    if dim_reduction == "PCA":
        st.header("ğŸ“Š PCA ë¶„ì„ ê²°ê³¼")
        
        # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
        fig_var = go.Figure()
        fig_var.add_trace(go.Bar(
            x=[f'PC{i+1}' for i in range(len(explained_var))],
            y=explained_var,
            name='ê°œë³„ ì„¤ëª…ëœ ë¶„ì‚°',
            marker_color='lightblue'
        ))
        fig_var.add_trace(go.Scatter(
            x=[f'PC{i+1}' for i in range(len(cumulative_var))],
            y=cumulative_var,
            mode='lines+markers',
            name='ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚°',
            yaxis='y2',
            line=dict(color='red')
        ))
        
        fig_var.update_layout(
            title="PCA ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨",
            xaxis_title="ì£¼ì„±ë¶„",
            yaxis_title="ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨",
            yaxis2=dict(
                title="ëˆ„ì  ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨",
                overlaying='y',
                side='right'
            ),
            height=400
        )
        
        st.plotly_chart(fig_var, use_container_width=True)
        add_chart_export_section(fig_var, "pca_explained_variance")
        
        # PCA ë¡œë”© í–‰ë ¬
        if n_components <= 3:
            st.subheader("PCA ë¡œë”© í–‰ë ¬")
            loadings = pca_model.components_.T
            loading_df = pd.DataFrame(
                loadings,
                columns=[f'PC{i+1}' for i in range(n_components)],
                index=selected_features
            )
            st.dataframe(loading_df.round(4))
    
    # í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ì„ íƒ
    st.sidebar.header("ğŸ² í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜")
    clustering_method = st.sidebar.selectbox(
        "í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²•",
        ["K-Means", "DBSCAN", "Gaussian Mixture", "Agglomerative Clustering"]
    )
    
    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    st.header("ğŸ” í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    
    if clustering_method == "K-Means":
        st.subheader("K-Means í´ëŸ¬ìŠ¤í„°ë§")
        
        # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
        if st.checkbox("ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¶„ì„"):
            max_k = st.slider("ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜", 3, 15, 10, key="clustering_max_k")
            
            with st.spinner("ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ë¥¼ ë¶„ì„ ì¤‘..."):
                k_range, inertias, silhouette_scores = find_optimal_clusters(X_reduced, max_k, 'kmeans')
            
            if k_range is not None:
                # ì—˜ë³´ìš° ë°©ë²•ê³¼ ì‹¤ë£¨ì—£ ì ìˆ˜
                fig_elbow = make_subplots(
                    rows=1, cols=2,
                    subplot_titles=['ì—˜ë³´ìš° ë°©ë²• (Inertia)', 'ì‹¤ë£¨ì—£ ì ìˆ˜'],
                    specs=[[{"secondary_y": False}, {"secondary_y": False}]]
                )
                
                # ì—˜ë³´ìš° ë°©ë²•
                fig_elbow.add_trace(
                    go.Scatter(x=list(k_range), y=inertias, mode='lines+markers',
                              name='Inertia', line=dict(color='blue')),
                    row=1, col=1
                )
                
                # ì‹¤ë£¨ì—£ ì ìˆ˜
                fig_elbow.add_trace(
                    go.Scatter(x=list(k_range), y=silhouette_scores, mode='lines+markers',
                              name='Silhouette Score', line=dict(color='red')),
                    row=1, col=2
                )
                
                fig_elbow.update_layout(height=400, showlegend=False)
                fig_elbow.update_xaxes(title_text="í´ëŸ¬ìŠ¤í„° ìˆ˜ (k)")
                
                st.plotly_chart(fig_elbow, use_container_width=True)
                add_chart_export_section(fig_elbow, "kmeans_optimal_clusters")
                
                # ì¶”ì²œ í´ëŸ¬ìŠ¤í„° ìˆ˜
                best_k_silhouette = k_range[np.argmax(silhouette_scores)]
                st.info(f"ì‹¤ë£¨ì—£ ì ìˆ˜ ê¸°ì¤€ ì¶”ì²œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {best_k_silhouette}")
        
        # K-Means íŒŒë¼ë¯¸í„°
        n_clusters = st.slider("í´ëŸ¬ìŠ¤í„° ìˆ˜", 2, 10, 3, key="kmeans_n_clusters")
        random_state = st.number_input("ëœë¤ ì‹œë“œ", 0, 1000, 42)
        
        # K-Means ì‹¤í–‰
        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
        cluster_labels = kmeans.fit_predict(X_reduced)
        
        # ì„±ëŠ¥ ì§€í‘œ
        silhouette_avg = silhouette_score(X_reduced, cluster_labels)
        calinski_harabasz = calinski_harabasz_score(X_reduced, cluster_labels)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í´ëŸ¬ìŠ¤í„° ìˆ˜", n_clusters)
        with col2:
            st.metric("ì‹¤ë£¨ì—£ ì ìˆ˜", f"{silhouette_avg:.4f}")
        with col3:
            st.metric("Calinski-Harabasz ì§€ìˆ˜", f"{calinski_harabasz:.2f}")
    
    elif clustering_method == "DBSCAN":
        st.subheader("DBSCAN í´ëŸ¬ìŠ¤í„°ë§")
        
        # DBSCAN íŒŒë¼ë¯¸í„°
        eps = st.slider("Epsilon (eps)", 0.1, 2.0, 0.5, 0.1, key="dbscan_eps")
        min_samples = st.slider("ìµœì†Œ ìƒ˜í”Œ ìˆ˜", 2, 20, 5, key="dbscan_min_samples")
        
        # DBSCAN ì‹¤í–‰
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        cluster_labels = dbscan.fit_predict(X_reduced)
        
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ìˆ˜
        n_noise = list(cluster_labels).count(-1)
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„° ìˆ˜", n_clusters)
        with col2:
            st.metric("ë…¸ì´ì¦ˆ í¬ì¸íŠ¸", n_noise)
        with col3:
            if n_clusters > 1:
                # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì œì™¸í•˜ê³  ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
                mask = cluster_labels != -1
                if np.sum(mask) > 1 and len(set(cluster_labels[mask])) > 1:
                    silhouette_avg = silhouette_score(X_reduced[mask], cluster_labels[mask])
                    st.metric("ì‹¤ë£¨ì—£ ì ìˆ˜", f"{silhouette_avg:.4f}")
                else:
                    st.metric("ì‹¤ë£¨ì—£ ì ìˆ˜", "ê³„ì‚° ë¶ˆê°€")
            else:
                st.metric("ì‹¤ë£¨ì—£ ì ìˆ˜", "ê³„ì‚° ë¶ˆê°€")
    
    elif clustering_method == "Gaussian Mixture":
        st.subheader("Gaussian Mixture ëª¨ë¸")
        
        # GMM íŒŒë¼ë¯¸í„°
        n_components_gmm = st.slider("ì»´í¬ë„ŒíŠ¸ ìˆ˜", 2, 10, 3, key="gmm_n_components")
        covariance_type = st.selectbox("ê³µë¶„ì‚° íƒ€ì…", ["full", "tied", "diag", "spherical"])
        
        # GMM ì‹¤í–‰
        gmm = GaussianMixture(n_components=n_components_gmm, covariance_type=covariance_type, random_state=42)
        cluster_labels = gmm.fit_predict(X_reduced)
        
        # ì„±ëŠ¥ ì§€í‘œ
        silhouette_avg = silhouette_score(X_reduced, cluster_labels)
        aic = gmm.aic(X_reduced)
        bic = gmm.bic(X_reduced)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì»´í¬ë„ŒíŠ¸ ìˆ˜", n_components_gmm)
        with col2:
            st.metric("ì‹¤ë£¨ì—£ ì ìˆ˜", f"{silhouette_avg:.4f}")
        with col3:
            st.metric("AIC", f"{aic:.2f}")
        with col4:
            st.metric("BIC", f"{bic:.2f}")
    
    else:  # Agglomerative Clustering
        st.subheader("ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§")
        
        # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„°
        n_clusters_agg = st.slider("í´ëŸ¬ìŠ¤í„° ìˆ˜", 2, 10, 3, key="agg_n_clusters")
        linkage = st.selectbox("ì—°ê²° ë°©ë²•", ["ward", "complete", "average", "single"])
        
        # ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        agg_clustering = AgglomerativeClustering(n_clusters=n_clusters_agg, linkage=linkage)
        cluster_labels = agg_clustering.fit_predict(X_reduced)
        
        # ì„±ëŠ¥ ì§€í‘œ
        silhouette_avg = silhouette_score(X_reduced, cluster_labels)
        calinski_harabasz = calinski_harabasz_score(X_reduced, cluster_labels)
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í´ëŸ¬ìŠ¤í„° ìˆ˜", n_clusters_agg)
        with col2:
            st.metric("ì‹¤ë£¨ì—£ ì ìˆ˜", f"{silhouette_avg:.4f}")
        with col3:
            st.metric("Calinski-Harabasz ì§€ìˆ˜", f"{calinski_harabasz:.2f}")
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
    st.subheader("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”")
    
    if X_reduced.shape[1] >= 2:
        title = f"{clustering_method} í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼"
        if dim_reduction != "ì—†ìŒ":
            title += f" ({reduction_method_name})"
        
        cluster_fig = plot_cluster_results(
            X_reduced[:, :min(3, X_reduced.shape[1])], 
            cluster_labels, 
            title,
            selected_features,
            reduction_method_name
        )
        
        st.plotly_chart(cluster_fig, use_container_width=True)
        add_chart_export_section(cluster_fig, f"{clustering_method.lower()}_clustering_result")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„
    st.subheader("ğŸ“ˆ í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„")
    
    # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì„ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€
    cluster_df = X.copy()
    cluster_df['Cluster'] = cluster_labels
    
    # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
    cluster_stats = cluster_df.groupby('Cluster')[selected_features].agg(['mean', 'std', 'count'])
    
    st.markdown("**í´ëŸ¬ìŠ¤í„°ë³„ í‰ê· ê°’**")
    mean_stats = cluster_stats.xs('mean', level=1, axis=1)
    st.dataframe(mean_stats.round(4))
    
    # í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± íˆíŠ¸ë§µ
    if len(selected_features) > 1:
        fig_heatmap = px.imshow(
            mean_stats.T,
            labels=dict(x="í´ëŸ¬ìŠ¤í„°", y="í”¼ì²˜", color="í‰ê· ê°’"),
            x=[f"í´ëŸ¬ìŠ¤í„° {i}" for i in mean_stats.index],
            y=selected_features,
            color_continuous_scale='RdYlBu_r',
            title="í´ëŸ¬ìŠ¤í„°ë³„ í”¼ì²˜ í‰ê· ê°’ íˆíŠ¸ë§µ"
        )
        fig_heatmap.update_layout(height=400)
        
        st.plotly_chart(fig_heatmap, use_container_width=True)
        add_chart_export_section(fig_heatmap, "cluster_characteristics_heatmap")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬ ì‹œê°í™”
    st.subheader("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ í”¼ì²˜ ë¶„í¬")
    
    # í”¼ì²˜ ì„ íƒ
    feature_for_dist = st.selectbox("ë¶„í¬ë¥¼ ë³¼ í”¼ì²˜ ì„ íƒ", selected_features)
    
    # ë°•ìŠ¤í”Œë¡¯
    fig_box = px.box(
        cluster_df, 
        x='Cluster', 
        y=feature_for_dist,
        title=f"{feature_for_dist}ì˜ í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬",
        color='Cluster',
        color_discrete_sequence=px.colors.qualitative.Set3
    )
    fig_box.update_layout(height=400)
    
    st.plotly_chart(fig_box, use_container_width=True)
    add_chart_export_section(fig_box, f"cluster_distribution_{feature_for_dist}")
    
    # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„ì„
    st.subheader("ğŸ“ í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„ì„")
    
    cluster_sizes = cluster_df['Cluster'].value_counts().sort_index()
    
    # íŒŒì´ ì°¨íŠ¸
    fig_pie = px.pie(
        values=cluster_sizes.values,
        names=[f"í´ëŸ¬ìŠ¤í„° {i}" for i in cluster_sizes.index],
        title="í´ëŸ¬ìŠ¤í„°ë³„ ë°ì´í„° í¬ì¸íŠ¸ ë¶„í¬"
    )
    fig_pie.update_layout(height=400)
    
    st.plotly_chart(fig_pie, use_container_width=True)
    add_chart_export_section(fig_pie, "cluster_size_distribution")
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½ í†µê³„
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜", len(cluster_sizes))
    with col2:
        st.metric("ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° í¬ê¸°", cluster_sizes.max())
    
    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
    st.subheader("ğŸ’¾ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
    
    # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    result_df = df.copy()
    if dim_reduction == "t-SNE":
        result_df = result_df.iloc[sample_idx].copy()
    
    result_df['Cluster'] = cluster_labels
    
    if X_reduced.shape[1] <= 3:
        for i in range(X_reduced.shape[1]):
            result_df[f'{reduction_method_name}_Component_{i+1}'] = X_reduced[:, i]
    
    # CSV ë‹¤ìš´ë¡œë“œ
    csv = result_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv,
        file_name=f"clustering_results_{clustering_method.lower()}.csv",
        mime="text/csv"
    )
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ë¶„ì„
    with st.expander("ğŸ” í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ë¶„ì„"):
        selected_cluster = st.selectbox(
            "ë¶„ì„í•  í´ëŸ¬ìŠ¤í„° ì„ íƒ",
            sorted(cluster_df['Cluster'].unique())
        )
        
        cluster_data = cluster_df[cluster_df['Cluster'] == selected_cluster]
        
        st.subheader(f"í´ëŸ¬ìŠ¤í„° {selected_cluster} ìƒì„¸ ì •ë³´")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜", len(cluster_data))
        with col2:
            st.metric("ì „ì²´ ë°ì´í„° ë¹„ìœ¨", f"{len(cluster_data)/len(cluster_df)*100:.1f}%")
        with col3:
            if clustering_method == "DBSCAN" and selected_cluster == -1:
                st.metric("í´ëŸ¬ìŠ¤í„° íƒ€ì…", "ë…¸ì´ì¦ˆ")
            else:
                st.metric("í´ëŸ¬ìŠ¤í„° íƒ€ì…", "ì •ìƒ")
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ í†µê³„
        st.markdown("**í´ëŸ¬ìŠ¤í„° ë‚´ í”¼ì²˜ í†µê³„**")
        cluster_stats_detail = cluster_data[selected_features].describe()
        st.dataframe(cluster_stats_detail.round(4))
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° ìƒ˜í”Œ
        st.markdown("**í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° ìƒ˜í”Œ**")
        st.dataframe(cluster_data.head(10))

if __name__ == "__main__":
    main() 