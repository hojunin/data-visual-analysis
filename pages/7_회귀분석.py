import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from scipy import stats
import sys
import os

# utils.py ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils import add_dark_mode_toggle, add_chart_export_section, style_metric_cards

st.title("ğŸ“ˆ íšŒê·€ë¶„ì„ ì „ë¬¸ ë„êµ¬")

# ë‹¤í¬ëª¨ë“œ í† ê¸€ ë° ìŠ¤íƒ€ì¼ ì¶”ê°€
add_dark_mode_toggle()
style_metric_cards()
st.markdown("ë‹¤ì–‘í•œ íšŒê·€ë¶„ì„ ëª¨ë¸ì„ ë¹„êµí•˜ê³  ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ ë„êµ¬ì…ë‹ˆë‹¤.")

uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    st.write("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", df.head())

    numeric_cols = df.select_dtypes(include='number').columns.tolist()
    
    if len(numeric_cols) >= 2:
        # ì‚¬ì´ë“œë°”ì—ì„œ íšŒê·€ë¶„ì„ ì„¤ì •
        st.sidebar.header("íšŒê·€ë¶„ì„ ì„¤ì •")
        
        regression_type = st.sidebar.selectbox(
            "íšŒê·€ë¶„ì„ ìœ í˜•",
            ["ë‹¨ìˆœ ì„ í˜•íšŒê·€", "ë‹¤ì¤‘ ì„ í˜•íšŒê·€", "ë‹¤í•­ íšŒê·€", "ì •ê·œí™” íšŒê·€", "ëª¨ë¸ ë¹„êµ"]
        )
        
        # ì¢…ì†ë³€ìˆ˜ ì„ íƒ
        target_var = st.sidebar.selectbox("ì¢…ì†ë³€ìˆ˜ (Y)", numeric_cols, key="target")
        
        # ë…ë¦½ë³€ìˆ˜ ì„ íƒ
        available_features = [col for col in numeric_cols if col != target_var]
        
        if regression_type == "ë‹¨ìˆœ ì„ í˜•íšŒê·€":
            st.markdown("## ğŸ“Š ë‹¨ìˆœ ì„ í˜•íšŒê·€ë¶„ì„")
            st.markdown("í•˜ë‚˜ì˜ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
            
            feature_var = st.selectbox("ë…ë¦½ë³€ìˆ˜ (X)", available_features)
            
            # ë°ì´í„° ì¤€ë¹„
            X = df[[feature_var]].dropna()
            y = df[target_var].dropna()
            
            # ê³µí†µ ì¸ë±ìŠ¤
            common_idx = X.index.intersection(y.index)
            X_clean = X.loc[common_idx]
            y_clean = y.loc[common_idx]
            
            # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
            test_size = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05)
            X_train, X_test, y_train, y_test = train_test_split(
                X_clean, y_clean, test_size=test_size, random_state=42
            )
            
            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            y_train_pred = model.predict(X_train)
            y_test_pred = model.predict(X_test)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                train_r2 = r2_score(y_train, y_train_pred)
                st.metric("í›ˆë ¨ RÂ²", f"{train_r2:.4f}")
            
            with col2:
                test_r2 = r2_score(y_test, y_test_pred)
                st.metric("í…ŒìŠ¤íŠ¸ RÂ²", f"{test_r2:.4f}")
            
            with col3:
                train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
                st.metric("í›ˆë ¨ RMSE", f"{train_rmse:.4f}")
            
            with col4:
                test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
                st.metric("í…ŒìŠ¤íŠ¸ RMSE", f"{test_rmse:.4f}")
            
            # íšŒê·€ ë°©ì •ì‹
            coef = model.coef_[0]
            intercept = model.intercept_
            st.markdown(f"**íšŒê·€ ë°©ì •ì‹**: {target_var} = {coef:.4f} Ã— {feature_var} + {intercept:.4f}")
            
            # ì‹œê°í™”
            col1, col2 = st.columns(2)
            
            with col1:
                # ì‚°ì ë„ì™€ íšŒê·€ì„ 
                fig_scatter = px.scatter(
                    x=X_clean[feature_var], y=y_clean, 
                    title=f"{target_var} vs {feature_var}",
                    labels={'x': feature_var, 'y': target_var}
                )
                
                # íšŒê·€ì„  ì¶”ê°€
                x_range = np.linspace(X_clean[feature_var].min(), X_clean[feature_var].max(), 100)
                y_pred_line = model.predict(x_range.reshape(-1, 1))
                
                fig_scatter.add_trace(go.Scatter(
                    x=x_range, y=y_pred_line,
                    mode='lines', name='íšŒê·€ì„ ',
                    line=dict(color='red', width=2)
                ))
                
                st.plotly_chart(fig_scatter, use_container_width=True)
                add_chart_export_section(fig_scatter, f"simple_regression_{feature_var}_{target_var}")
            
            with col2:
                # ì”ì°¨ í”Œë¡¯
                residuals = y_test - y_test_pred
                fig_residual = px.scatter(
                    x=y_test_pred, y=residuals,
                    title="ì”ì°¨ í”Œë¡¯ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)",
                    labels={'x': 'ì˜ˆì¸¡ê°’', 'y': 'ì”ì°¨'}
                )
                fig_residual.add_hline(y=0, line_dash="dash", line_color="red")
                st.plotly_chart(fig_residual, use_container_width=True)
                add_chart_export_section(fig_residual, f"residuals_{feature_var}_{target_var}")
            
            # íšŒê·€ ì§„ë‹¨
            st.markdown("### ğŸ” íšŒê·€ ì§„ë‹¨")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # ì •ê·œì„± ê²€ì • (ì”ì°¨)
                shapiro_stat, shapiro_p = stats.shapiro(residuals)
                st.metric("Shapiro-Wilk p-value", f"{shapiro_p:.4f}")
                if shapiro_p > 0.05:
                    st.success("ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤")
                else:
                    st.warning("ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            with col2:
                # Durbin-Watson ê²€ì • (ìê¸°ìƒê´€)
                from statsmodels.stats.diagnostic import durbin_watson
                dw_stat = durbin_watson(residuals)
                st.metric("Durbin-Watson", f"{dw_stat:.4f}")
                if 1.5 <= dw_stat <= 2.5:
                    st.success("ìê¸°ìƒê´€ì´ ì—†ìŠµë‹ˆë‹¤")
                else:
                    st.warning("ìê¸°ìƒê´€ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            
            with col3:
                # ìƒê´€ê³„ìˆ˜
                correlation = np.corrcoef(X_clean[feature_var], y_clean)[0, 1]
                st.metric("ìƒê´€ê³„ìˆ˜", f"{correlation:.4f}")
        
        elif regression_type == "ë‹¤ì¤‘ ì„ í˜•íšŒê·€":
            st.markdown("## ğŸ“Š ë‹¤ì¤‘ ì„ í˜•íšŒê·€ë¶„ì„")
            st.markdown("ì—¬ëŸ¬ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
            
            # ë…ë¦½ë³€ìˆ˜ ì„ íƒ
            selected_features = st.multiselect(
                "ë…ë¦½ë³€ìˆ˜ë“¤ ì„ íƒ",
                available_features,
                default=available_features[:min(3, len(available_features))]
            )
            
            if len(selected_features) >= 1:
                # ë°ì´í„° ì¤€ë¹„
                X = df[selected_features].dropna()
                y = df[target_var].dropna()
                
                # ê³µí†µ ì¸ë±ìŠ¤
                common_idx = X.index.intersection(y.index)
                X_clean = X.loc[common_idx]
                y_clean = y.loc[common_idx]
                
                # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                test_size = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05, key="multi_test_size")
                X_train, X_test, y_train, y_test = train_test_split(
                    X_clean, y_clean, test_size=test_size, random_state=42
                )
                
                # ëª¨ë¸ í•™ìŠµ
                model = LinearRegression()
                model.fit(X_train, y_train)
                
                # ì˜ˆì¸¡
                y_train_pred = model.predict(X_train)
                y_test_pred = model.predict(X_test)
                
                # ì„±ëŠ¥ ë©”íŠ¸ë¦­
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    train_r2 = r2_score(y_train, y_train_pred)
                    st.metric("í›ˆë ¨ RÂ²", f"{train_r2:.4f}")
                
                with col2:
                    test_r2 = r2_score(y_test, y_test_pred)
                    st.metric("í…ŒìŠ¤íŠ¸ RÂ²", f"{test_r2:.4f}")
                
                with col3:
                    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
                    st.metric("í›ˆë ¨ RMSE", f"{train_rmse:.4f}")
                
                with col4:
                    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
                    st.metric("í…ŒìŠ¤íŠ¸ RMSE", f"{test_rmse:.4f}")
                
                # íšŒê·€ ê³„ìˆ˜
                st.markdown("### ğŸ“Š íšŒê·€ ê³„ìˆ˜")
                coef_df = pd.DataFrame({
                    'ë³€ìˆ˜': selected_features,
                    'ê³„ìˆ˜': model.coef_,
                    'ì ˆëŒ“ê°’': np.abs(model.coef_)
                }).sort_values('ì ˆëŒ“ê°’', ascending=False)
                
                st.dataframe(coef_df.round(4))
                
                # íšŒê·€ ë°©ì •ì‹
                equation = f"{target_var} = "
                for i, feature in enumerate(selected_features):
                    if i == 0:
                        equation += f"{model.coef_[i]:.4f} Ã— {feature}"
                    else:
                        sign = "+" if model.coef_[i] >= 0 else ""
                        equation += f" {sign} {model.coef_[i]:.4f} Ã— {feature}"
                equation += f" + {model.intercept_:.4f}"
                
                st.markdown(f"**íšŒê·€ ë°©ì •ì‹**: {equation}")
                
                # ê³„ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”
                fig_coef = px.bar(
                    coef_df, x='ì ˆëŒ“ê°’', y='ë³€ìˆ˜', orientation='h',
                    title="ë³€ìˆ˜ ì¤‘ìš”ë„ (ê³„ìˆ˜ ì ˆëŒ“ê°’)"
                )
                st.plotly_chart(fig_coef, use_container_width=True)
                add_chart_export_section(fig_coef, f"coefficients_{target_var}")
                
                # ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’
                col1, col2 = st.columns(2)
                
                with col1:
                    fig_pred = px.scatter(
                        x=y_test, y=y_test_pred,
                        title="ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)"
                    )
                    # ëŒ€ê°ì„  ì¶”ê°€ (ì™„ë²½í•œ ì˜ˆì¸¡)
                    min_val = min(y_test.min(), y_test_pred.min())
                    max_val = max(y_test.max(), y_test_pred.max())
                    fig_pred.add_trace(go.Scatter(
                        x=[min_val, max_val], y=[min_val, max_val],
                        mode='lines', name='ì™„ë²½í•œ ì˜ˆì¸¡',
                        line=dict(color='red', dash='dash')
                    ))
                    st.plotly_chart(fig_pred, use_container_width=True)
                
                with col2:
                    # ì”ì°¨ í”Œë¡¯
                    residuals = y_test - y_test_pred
                    fig_residual = px.scatter(
                        x=y_test_pred, y=residuals,
                        title="ì”ì°¨ í”Œë¡¯"
                    )
                    fig_residual.add_hline(y=0, line_dash="dash", line_color="red")
                    st.plotly_chart(fig_residual, use_container_width=True)
                
                # êµì°¨ê²€ì¦
                st.markdown("### ğŸ”„ êµì°¨ê²€ì¦")
                cv_scores = cross_val_score(model, X_clean, y_clean, cv=5, scoring='r2')
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("CV í‰ê·  RÂ²", f"{cv_scores.mean():.4f}")
                with col2:
                    st.metric("CV í‘œì¤€í¸ì°¨", f"{cv_scores.std():.4f}")
                with col3:
                    st.metric("CV ì ìˆ˜ ë²”ìœ„", f"{cv_scores.min():.3f} ~ {cv_scores.max():.3f}")
        
        elif regression_type == "ë‹¤í•­ íšŒê·€":
            st.markdown("## ğŸ“Š ë‹¤í•­ íšŒê·€ë¶„ì„")
            st.markdown("ë¹„ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ê¸° ìœ„í•œ ë‹¤í•­ íšŒê·€ë¶„ì„ì…ë‹ˆë‹¤.")
            
            feature_var = st.selectbox("ë…ë¦½ë³€ìˆ˜ (X)", available_features, key="poly_feature")
            degree = st.slider("ë‹¤í•­ì‹ ì°¨ìˆ˜", 1, 5, 2)
            
            # ë°ì´í„° ì¤€ë¹„
            X = df[[feature_var]].dropna()
            y = df[target_var].dropna()
            
            common_idx = X.index.intersection(y.index)
            X_clean = X.loc[common_idx]
            y_clean = y.loc[common_idx]
            
            # ë‹¤í•­ íŠ¹ì„± ìƒì„±
            poly_features = PolynomialFeatures(degree=degree, include_bias=False)
            X_poly = poly_features.fit_transform(X_clean)
            
            # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
            test_size = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05, key="poly_test_size")
            X_train, X_test, y_train, y_test = train_test_split(
                X_poly, y_clean, test_size=test_size, random_state=42
            )
            
            # ëª¨ë¸ í•™ìŠµ
            model = LinearRegression()
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡
            y_train_pred = model.predict(X_train)
            y_test_pred = model.predict(X_test)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                train_r2 = r2_score(y_train, y_train_pred)
                st.metric("í›ˆë ¨ RÂ²", f"{train_r2:.4f}")
            
            with col2:
                test_r2 = r2_score(y_test, y_test_pred)
                st.metric("í…ŒìŠ¤íŠ¸ RÂ²", f"{test_r2:.4f}")
            
            with col3:
                train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
                st.metric("í›ˆë ¨ RMSE", f"{train_rmse:.4f}")
            
            with col4:
                test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
                st.metric("í…ŒìŠ¤íŠ¸ RMSE", f"{test_rmse:.4f}")
            
            # ê³¼ì í•© ê²½ê³ 
            if train_r2 - test_r2 > 0.1:
                st.warning("âš ï¸ ê³¼ì í•©ì´ ì˜ì‹¬ë©ë‹ˆë‹¤. ì°¨ìˆ˜ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”.")
            
            # ì‹œê°í™”
            fig = px.scatter(
                x=X_clean[feature_var], y=y_clean,
                title=f"{degree}ì°¨ ë‹¤í•­ íšŒê·€: {target_var} vs {feature_var}"
            )
            
            # ë‹¤í•­ íšŒê·€ ê³¡ì„  ê·¸ë¦¬ê¸°
            x_range = np.linspace(X_clean[feature_var].min(), X_clean[feature_var].max(), 300)
            X_range_poly = poly_features.transform(x_range.reshape(-1, 1))
            y_range_pred = model.predict(X_range_poly)
            
            fig.add_trace(go.Scatter(
                x=x_range, y=y_range_pred,
                mode='lines', name=f'{degree}ì°¨ ë‹¤í•­ íšŒê·€',
                line=dict(color='red', width=2)
            ))
            
            st.plotly_chart(fig, use_container_width=True)
            add_chart_export_section(fig, f"polynomial_regression_degree_{degree}")
            
            # ì°¨ìˆ˜ë³„ ì„±ëŠ¥ ë¹„êµ
            st.markdown("### ğŸ“Š ì°¨ìˆ˜ë³„ ì„±ëŠ¥ ë¹„êµ")
            
            degrees = range(1, 6)
            train_scores = []
            test_scores = []
            
            X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
                X_clean, y_clean, test_size=test_size, random_state=42
            )
            
            for d in degrees:
                poly = PolynomialFeatures(degree=d, include_bias=False)
                X_train_poly = poly.fit_transform(X_train_orig)
                X_test_poly = poly.transform(X_test_orig)
                
                temp_model = LinearRegression()
                temp_model.fit(X_train_poly, y_train_orig)
                
                train_pred = temp_model.predict(X_train_poly)
                test_pred = temp_model.predict(X_test_poly)
                
                train_scores.append(r2_score(y_train_orig, train_pred))
                test_scores.append(r2_score(y_test_orig, test_pred))
            
            fig_comparison = go.Figure()
            fig_comparison.add_trace(go.Scatter(
                x=list(degrees), y=train_scores,
                mode='lines+markers', name='í›ˆë ¨ RÂ²',
                line=dict(color='blue')
            ))
            fig_comparison.add_trace(go.Scatter(
                x=list(degrees), y=test_scores,
                mode='lines+markers', name='í…ŒìŠ¤íŠ¸ RÂ²',
                line=dict(color='red')
            ))
            
            fig_comparison.update_layout(
                title="ë‹¤í•­ì‹ ì°¨ìˆ˜ë³„ ì„±ëŠ¥ ë¹„êµ",
                xaxis_title="ë‹¤í•­ì‹ ì°¨ìˆ˜",
                yaxis_title="RÂ² ì ìˆ˜"
            )
            
            st.plotly_chart(fig_comparison, use_container_width=True)
            add_chart_export_section(fig_comparison, "polynomial_degree_comparison")
        
        elif regression_type == "ì •ê·œí™” íšŒê·€":
            st.markdown("## ğŸ“Š ì •ê·œí™” íšŒê·€ë¶„ì„")
            st.markdown("ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” Ridge, Lasso, ElasticNet íšŒê·€ë¶„ì„ì…ë‹ˆë‹¤.")
            
            # ë…ë¦½ë³€ìˆ˜ ì„ íƒ
            selected_features = st.multiselect(
                "ë…ë¦½ë³€ìˆ˜ë“¤ ì„ íƒ",
                available_features,
                default=available_features[:min(5, len(available_features))],
                key="regularization_features"
            )
            
            if len(selected_features) >= 1:
                # ë°ì´í„° ì¤€ë¹„
                X = df[selected_features].dropna()
                y = df[target_var].dropna()
                
                common_idx = X.index.intersection(y.index)
                X_clean = X.loc[common_idx]
                y_clean = y.loc[common_idx]
                
                # ë°ì´í„° í‘œì¤€í™”
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_clean)
                
                # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                test_size = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05, key="reg_test_size")
                X_train, X_test, y_train, y_test = train_test_split(
                    X_scaled, y_clean, test_size=test_size, random_state=42
                )
                
                # ì •ê·œí™” íŒŒë¼ë¯¸í„°
                alpha = st.slider("ì •ê·œí™” ê°•ë„ (alpha)", 0.01, 10.0, 1.0, 0.01)
                
                # ëª¨ë¸ë“¤ í•™ìŠµ
                models = {
                    'Linear': LinearRegression(),
                    'Ridge': Ridge(alpha=alpha),
                    'Lasso': Lasso(alpha=alpha),
                    'ElasticNet': ElasticNet(alpha=alpha, l1_ratio=0.5)
                }
                
                results = {}
                
                for name, model in models.items():
                    model.fit(X_train, y_train)
                    
                    y_train_pred = model.predict(X_train)
                    y_test_pred = model.predict(X_test)
                    
                    results[name] = {
                        'train_r2': r2_score(y_train, y_train_pred),
                        'test_r2': r2_score(y_test, y_test_pred),
                        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
                        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),
                        'model': model
                    }
                
                # ê²°ê³¼ ë¹„êµ í…Œì´ë¸”
                st.markdown("### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
                
                comparison_df = pd.DataFrame({
                    'ëª¨ë¸': list(results.keys()),
                    'í›ˆë ¨ RÂ²': [results[name]['train_r2'] for name in results.keys()],
                    'í…ŒìŠ¤íŠ¸ RÂ²': [results[name]['test_r2'] for name in results.keys()],
                    'í›ˆë ¨ RMSE': [results[name]['train_rmse'] for name in results.keys()],
                    'í…ŒìŠ¤íŠ¸ RMSE': [results[name]['test_rmse'] for name in results.keys()]
                }).round(4)
                
                st.dataframe(comparison_df)
                
                # ì„±ëŠ¥ ì‹œê°í™”
                fig_comparison = go.Figure()
                
                fig_comparison.add_trace(go.Bar(
                    name='í›ˆë ¨ RÂ²',
                    x=list(results.keys()),
                    y=[results[name]['train_r2'] for name in results.keys()],
                    marker_color='lightblue'
                ))
                
                fig_comparison.add_trace(go.Bar(
                    name='í…ŒìŠ¤íŠ¸ RÂ²',
                    x=list(results.keys()),
                    y=[results[name]['test_r2'] for name in results.keys()],
                    marker_color='lightcoral'
                ))
                
                fig_comparison.update_layout(
                    title="ëª¨ë¸ë³„ RÂ² ì„±ëŠ¥ ë¹„êµ",
                    xaxis_title="ëª¨ë¸",
                    yaxis_title="RÂ² ì ìˆ˜",
                    barmode='group'
                )
                
                st.plotly_chart(fig_comparison, use_container_width=True)
                add_chart_export_section(fig_comparison, f"regularization_comparison_alpha_{alpha}")
                
                # ê³„ìˆ˜ ë¹„êµ (Ridge vs Lasso)
                st.markdown("### ğŸ“Š íšŒê·€ ê³„ìˆ˜ ë¹„êµ")
                
                coef_comparison = pd.DataFrame({
                    'ë³€ìˆ˜': selected_features,
                    'Linear': results['Linear']['model'].coef_,
                    'Ridge': results['Ridge']['model'].coef_,
                    'Lasso': results['Lasso']['model'].coef_,
                    'ElasticNet': results['ElasticNet']['model'].coef_
                })
                
                st.dataframe(coef_comparison.round(4))
                
                # ê³„ìˆ˜ ì‹œê°í™”
                fig_coef = go.Figure()
                
                for model_name in ['Linear', 'Ridge', 'Lasso', 'ElasticNet']:
                    fig_coef.add_trace(go.Bar(
                        name=model_name,
                        x=selected_features,
                        y=coef_comparison[model_name]
                    ))
                
                fig_coef.update_layout(
                    title="ëª¨ë¸ë³„ íšŒê·€ ê³„ìˆ˜ ë¹„êµ",
                    xaxis_title="ë³€ìˆ˜",
                    yaxis_title="ê³„ìˆ˜ ê°’",
                    barmode='group'
                )
                
                st.plotly_chart(fig_coef, use_container_width=True)
                add_chart_export_section(fig_coef, "coefficient_comparison")
                
                # ì•ŒíŒŒ ê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
                st.markdown("### ğŸ“ˆ ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”")
                
                alphas = np.logspace(-3, 2, 20)  # 0.001 ~ 100
                ridge_scores = []
                lasso_scores = []
                
                for a in alphas:
                    ridge_model = Ridge(alpha=a)
                    lasso_model = Lasso(alpha=a)
                    
                    ridge_model.fit(X_train, y_train)
                    lasso_model.fit(X_train, y_train)
                    
                    ridge_scores.append(r2_score(y_test, ridge_model.predict(X_test)))
                    lasso_scores.append(r2_score(y_test, lasso_model.predict(X_test)))
                
                fig_alpha = go.Figure()
                fig_alpha.add_trace(go.Scatter(
                    x=alphas, y=ridge_scores,
                    mode='lines+markers', name='Ridge',
                    line=dict(color='blue')
                ))
                fig_alpha.add_trace(go.Scatter(
                    x=alphas, y=lasso_scores,
                    mode='lines+markers', name='Lasso',
                    line=dict(color='red')
                ))
                
                fig_alpha.update_layout(
                    title="ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ í…ŒìŠ¤íŠ¸ RÂ² ë³€í™”",
                    xaxis_title="Alpha (ë¡œê·¸ ìŠ¤ì¼€ì¼)",
                    yaxis_title="í…ŒìŠ¤íŠ¸ RÂ²",
                    xaxis_type="log"
                )
                
                st.plotly_chart(fig_alpha, use_container_width=True)
                add_chart_export_section(fig_alpha, "regularization_path")
        
        elif regression_type == "ëª¨ë¸ ë¹„êµ":
            st.markdown("## ğŸ“Š íšŒê·€ ëª¨ë¸ ì¢…í•© ë¹„êµ")
            st.markdown("ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í•œ ë²ˆì— ë¹„êµí•©ë‹ˆë‹¤.")
            
            # ë…ë¦½ë³€ìˆ˜ ì„ íƒ
            selected_features = st.multiselect(
                "ë…ë¦½ë³€ìˆ˜ë“¤ ì„ íƒ",
                available_features,
                default=available_features[:min(4, len(available_features))],
                key="comparison_features"
            )
            
            if len(selected_features) >= 1:
                # ë°ì´í„° ì¤€ë¹„
                X = df[selected_features].dropna()
                y = df[target_var].dropna()
                
                common_idx = X.index.intersection(y.index)
                X_clean = X.loc[common_idx]
                y_clean = y.loc[common_idx]
                
                # í‘œì¤€í™”
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_clean)
                
                # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
                test_size = st.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05, key="comp_test_size")
                X_train, X_test, y_train, y_test = train_test_split(
                    X_scaled, y_clean, test_size=test_size, random_state=42
                )
                
                # ë‹¤í•­ íŠ¹ì„± ìƒì„± (2ì°¨)
                poly_features = PolynomialFeatures(degree=2, include_bias=False)
                X_train_poly = poly_features.fit_transform(X_train)
                X_test_poly = poly_features.transform(X_test)
                
                # ëª¨ë¸ë“¤ ì •ì˜
                models = {
                    'ì„ í˜• íšŒê·€': LinearRegression(),
                    'Ridge íšŒê·€': Ridge(alpha=1.0),
                    'Lasso íšŒê·€': Lasso(alpha=1.0),
                    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5),
                    '2ì°¨ ë‹¤í•­ íšŒê·€': LinearRegression()
                }
                
                # ê²°ê³¼ ì €ì¥
                results = {}
                
                for name, model in models.items():
                    if name == '2ì°¨ ë‹¤í•­ íšŒê·€':
                        model.fit(X_train_poly, y_train)
                        y_train_pred = model.predict(X_train_poly)
                        y_test_pred = model.predict(X_test_poly)
                    else:
                        model.fit(X_train, y_train)
                        y_train_pred = model.predict(X_train)
                        y_test_pred = model.predict(X_test)
                    
                    # êµì°¨ê²€ì¦ ì ìˆ˜
                    if name == '2ì°¨ ë‹¤í•­ íšŒê·€':
                        X_for_cv = poly_features.fit_transform(X_scaled)
                        cv_scores = cross_val_score(LinearRegression(), X_for_cv, y_clean, cv=5, scoring='r2')
                    else:
                        cv_scores = cross_val_score(model, X_scaled, y_clean, cv=5, scoring='r2')
                    
                    results[name] = {
                        'train_r2': r2_score(y_train, y_train_pred),
                        'test_r2': r2_score(y_test, y_test_pred),
                        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
                        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),
                        'train_mae': mean_absolute_error(y_train, y_train_pred),
                        'test_mae': mean_absolute_error(y_test, y_test_pred),
                        'cv_mean': cv_scores.mean(),
                        'cv_std': cv_scores.std()
                    }
                
                # ì¢…í•© ê²°ê³¼ í…Œì´ë¸”
                st.markdown("### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ì¢…í•© ë¹„êµ")
                
                comparison_df = pd.DataFrame(results).T
                comparison_df = comparison_df.round(4)
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í‘œì‹œ
                best_test_r2 = comparison_df['test_r2'].max()
                best_model = comparison_df['test_r2'].idxmax()
                
                st.success(f"ğŸ† **ìµœê³  ì„±ëŠ¥ ëª¨ë¸**: {best_model} (í…ŒìŠ¤íŠ¸ RÂ² = {best_test_r2:.4f})")
                
                st.dataframe(comparison_df)
                
                # ì„±ëŠ¥ ì‹œê°í™” - RÂ² ë¹„êµ
                fig_r2 = go.Figure()
                
                fig_r2.add_trace(go.Bar(
                    name='í›ˆë ¨ RÂ²',
                    x=list(results.keys()),
                    y=[results[name]['train_r2'] for name in results.keys()],
                    marker_color='lightblue'
                ))
                
                fig_r2.add_trace(go.Bar(
                    name='í…ŒìŠ¤íŠ¸ RÂ²',
                    x=list(results.keys()),
                    y=[results[name]['test_r2'] for name in results.keys()],
                    marker_color='lightcoral'
                ))
                
                fig_r2.update_layout(
                    title="ëª¨ë¸ë³„ RÂ² ì„±ëŠ¥ ë¹„êµ",
                    xaxis_title="ëª¨ë¸",
                    yaxis_title="RÂ² ì ìˆ˜",
                    barmode='group'
                )
                
                st.plotly_chart(fig_r2, use_container_width=True)
                add_chart_export_section(fig_r2, "comprehensive_model_comparison")
                
                # RMSE ë¹„êµ
                fig_rmse = px.bar(
                    x=list(results.keys()),
                    y=[results[name]['test_rmse'] for name in results.keys()],
                    title="ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ RMSE ë¹„êµ"
                )
                st.plotly_chart(fig_rmse, use_container_width=True)
                
                # êµì°¨ê²€ì¦ ê²°ê³¼
                st.markdown("### ğŸ”„ êµì°¨ê²€ì¦ ê²°ê³¼")
                
                cv_df = pd.DataFrame({
                    'ëª¨ë¸': list(results.keys()),
                    'CV í‰ê·  RÂ²': [results[name]['cv_mean'] for name in results.keys()],
                    'CV í‘œì¤€í¸ì°¨': [results[name]['cv_std'] for name in results.keys()]
                }).round(4)
                
                st.dataframe(cv_df)
                
                # ê³¼ì í•© ë¶„ì„
                st.markdown("### ğŸ” ê³¼ì í•© ë¶„ì„")
                
                overfitting_df = pd.DataFrame({
                    'ëª¨ë¸': list(results.keys()),
                    'í›ˆë ¨ RÂ²': [results[name]['train_r2'] for name in results.keys()],
                    'í…ŒìŠ¤íŠ¸ RÂ²': [results[name]['test_r2'] for name in results.keys()],
                    'ì°¨ì´': [results[name]['train_r2'] - results[name]['test_r2'] for name in results.keys()]
                }).round(4)
                
                # ê³¼ì í•© ê²½ê³ 
                for idx, row in overfitting_df.iterrows():
                    if row['ì°¨ì´'] > 0.1:
                        st.warning(f"âš ï¸ {row['ëª¨ë¸']}: ê³¼ì í•© ê°€ëŠ¥ì„± (ì°¨ì´ = {row['ì°¨ì´']:.3f})")
                    elif row['ì°¨ì´'] < 0.05:
                        st.success(f"âœ… {row['ëª¨ë¸']}: ì•ˆì •ì ì¸ ì„±ëŠ¥")
                
                st.dataframe(overfitting_df)
    
    else:
        st.warning("íšŒê·€ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œ ì´ìƒì˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

else:
    st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ íšŒê·€ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”!")
    st.markdown("""
    ### íšŒê·€ë¶„ì„ ê¸°ëŠ¥ ë¯¸ë¦¬ë³´ê¸°
    
    **ë‹¨ìˆœ ì„ í˜•íšŒê·€**
    - í•˜ë‚˜ì˜ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ ë¶„ì„
    - íšŒê·€ì„  ì‹œê°í™” ë° ì”ì°¨ ë¶„ì„
    - íšŒê·€ ì§„ë‹¨ (ì •ê·œì„±, ìê¸°ìƒê´€ ê²€ì •)
    
    **ë‹¤ì¤‘ ì„ í˜•íšŒê·€**
    - ì—¬ëŸ¬ ë…ë¦½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ë§
    - ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„
    - êµì°¨ê²€ì¦ì„ í†µí•œ ëª¨ë¸ ì•ˆì •ì„± í‰ê°€
    
    **ë‹¤í•­ íšŒê·€**
    - ë¹„ì„ í˜• ê´€ê³„ ëª¨ë¸ë§
    - ì°¨ìˆ˜ë³„ ì„±ëŠ¥ ë¹„êµ
    - ê³¼ì í•© ë°©ì§€ ê¶Œì¥ì‚¬í•­
    
    **ì •ê·œí™” íšŒê·€**
    - Ridge, Lasso, ElasticNet íšŒê·€
    - ê³¼ì í•© ë°©ì§€ ë° íŠ¹ì„± ì„ íƒ
    - ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë¶„ì„
    
    **ëª¨ë¸ ë¹„êµ**
    - ëª¨ë“  íšŒê·€ ëª¨ë¸ì˜ ì¢…í•© ì„±ëŠ¥ ë¹„êµ
    - êµì°¨ê²€ì¦ ë° ê³¼ì í•© ë¶„ì„
    - ìµœì  ëª¨ë¸ ì¶”ì²œ
    """)

# TODO: ë¡œì§€ìŠ¤í‹± íšŒê·€, ì‹œê³„ì—´ íšŒê·€, ë² ì´ì§€ì•ˆ íšŒê·€ ì¶”ê°€ ì˜ˆì • 