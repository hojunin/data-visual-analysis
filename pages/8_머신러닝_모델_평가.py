import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve, validation_curve
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier
from sklearn.svm import SVR, SVC
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.append('..')
from utils import add_chart_export_section

def main():
    st.title("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í‰ê°€")
    st.markdown("ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ë¹„êµí•´ë³´ì„¸ìš”.")
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv'])
    
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©
        st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ìì‹ ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í•´ë³´ì„¸ìš”.")
        df = pd.read_csv('test.csv')
    
    st.sidebar.markdown("---")
    
    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
    if st.sidebar.checkbox("ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
        st.subheader("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df.head())
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í–‰ ìˆ˜", len(df))
        with col2:
            st.metric("ì—´ ìˆ˜", len(df.columns))
        with col3:
            st.metric("ê²°ì¸¡ê°’", df.isnull().sum().sum())
    
    # ëª¨ë¸ë§ ì„¤ì •
    st.sidebar.header("ğŸ¯ ëª¨ë¸ë§ ì„¤ì •")
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_columns) < 2:
        st.error("ìµœì†Œ 2ê°œ ì´ìƒì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ
    target_col = st.sidebar.selectbox("íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ", numeric_columns)
    
    # í”¼ì²˜ ë³€ìˆ˜ ì„ íƒ
    feature_cols = st.sidebar.multiselect(
        "í”¼ì²˜ ë³€ìˆ˜ ì„ íƒ", 
        [col for col in numeric_columns if col != target_col],
        default=[col for col in numeric_columns if col != target_col][:5]
    )
    
    if not feature_cols:
        st.warning("ìµœì†Œ í•˜ë‚˜ì˜ í”¼ì²˜ ë³€ìˆ˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return
    
    # ëª¨ë¸ ìœ í˜• ì„ íƒ
    model_type = st.sidebar.radio("ëª¨ë¸ ìœ í˜•", ["íšŒê·€", "ë¶„ë¥˜"])
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    X = df[feature_cols].dropna()
    y = df[target_col].dropna()
    
    # ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°
    common_index = X.index.intersection(y.index)
    X = X.loc[common_index]
    y = y.loc[common_index]
    
    if len(X) == 0:
        st.error("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°ì¸¡ê°’ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° íƒ€ê²Ÿ ë³€ìˆ˜ ì´ì§„í™”
    if model_type == "ë¶„ë¥˜":
        if len(y.unique()) > 10:
            # ì—°ì†í˜• ë³€ìˆ˜ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (ì¤‘ì•™ê°’ ê¸°ì¤€)
            threshold = y.median()
            y = (y > threshold).astype(int)
            st.info(f"íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì¤‘ì•™ê°’({threshold:.2f}) ê¸°ì¤€ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")
        else:
            # ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ ì¸ì½”ë”©
            le = LabelEncoder()
            y = le.fit_transform(y)
    
    # ë°ì´í„° ë¶„í• 
    test_size = st.sidebar.slider("í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨", 0.1, 0.5, 0.2, 0.05, key="ml_test_size")
    random_state = st.sidebar.number_input("ëœë¤ ì‹œë“œ", 0, 1000, 42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ëª¨ë¸ ì„ íƒ
    if model_type == "íšŒê·€":
        models = {
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=1.0),
            'Lasso Regression': Lasso(alpha=1.0),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=random_state),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=random_state),
            'SVM': SVR(kernel='rbf'),
            'Decision Tree': DecisionTreeRegressor(random_state=random_state),
            'KNN': KNeighborsRegressor(n_neighbors=5)
        }
        scoring = 'neg_mean_squared_error'
    else:
        models = {
            'Logistic Regression': LogisticRegression(random_state=random_state),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=random_state),
            'SVM': SVC(kernel='rbf', probability=True, random_state=random_state),
            'Decision Tree': DecisionTreeClassifier(random_state=random_state),
            'KNN': KNeighborsClassifier(n_neighbors=5),
            'Naive Bayes': GaussianNB()
        }
        scoring = 'accuracy'
    
    # ëª¨ë¸ ì„ íƒ
    selected_models = st.sidebar.multiselect(
        "ë¹„êµí•  ëª¨ë¸ ì„ íƒ",
        list(models.keys()),
        default=list(models.keys())[:4]
    )
    
    if not selected_models:
        st.warning("ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return
    
    # ë¶„ì„ ì‹œì‘
    st.header("ğŸ” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    results = {}
    trained_models = {}
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, model_name in enumerate(selected_models):
        status_text.text(f"ëª¨ë¸ í•™ìŠµ ì¤‘: {model_name}")
        
        model = models[model_name]
        
        # ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ëª¨ë¸ë“¤
        if model_name in ['SVM', 'Logistic Regression', 'KNN']:
            X_train_model = X_train_scaled
            X_test_model = X_test_scaled
        else:
            X_train_model = X_train
            X_test_model = X_test
        
        # ëª¨ë¸ í•™ìŠµ
        model.fit(X_train_model, y_train)
        trained_models[model_name] = model
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test_model)
        
        # êµì°¨ ê²€ì¦
        cv_scores = cross_val_score(
            model, X_train_model, y_train, 
            cv=5, scoring=scoring
        )
        
        if model_type == "íšŒê·€":
            # íšŒê·€ ëª¨ë¸ í‰ê°€
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            results[model_name] = {
                'MSE': mse,
                'MAE': mae,
                'RÂ²': r2,
                'RMSE': np.sqrt(mse),
                'CV Score (avg)': -cv_scores.mean(),
                'CV Score (std)': cv_scores.std(),
                'Predictions': y_pred
            }
        else:
            # ë¶„ë¥˜ ëª¨ë¸ í‰ê°€
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            results[model_name] = {
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1-Score': f1,
                'CV Score (avg)': cv_scores.mean(),
                'CV Score (std)': cv_scores.std(),
                'Predictions': y_pred
            }
        
        progress_bar.progress((i + 1) / len(selected_models))
    
    status_text.text("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    # ê²°ê³¼ í‘œì‹œ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ")
        results_df = pd.DataFrame(results).T
        st.dataframe(results_df.round(4))
    
    with col2:
        st.subheader("ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸")
        if model_type == "íšŒê·€":
            best_model = results_df['RÂ²'].idxmax()
            st.metric("ìµœê³  RÂ² ëª¨ë¸", best_model, f"{results_df.loc[best_model, 'RÂ²']:.4f}")
        else:
            best_model = results_df['Accuracy'].idxmax()
            st.metric("ìµœê³  ì •í™•ë„ ëª¨ë¸", best_model, f"{results_df.loc[best_model, 'Accuracy']:.4f}")
    
    # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
    st.subheader("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”")
    
    if model_type == "íšŒê·€":
        # íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['RÂ² Score', 'Mean Squared Error', 'Mean Absolute Error', 'Cross-Validation Scores'],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        models_list = list(results.keys())
        
        # RÂ² Score
        r2_scores = [results[m]['RÂ²'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=r2_scores, name='RÂ²', marker_color='lightblue'), row=1, col=1)
        
        # MSE
        mse_scores = [results[m]['MSE'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=mse_scores, name='MSE', marker_color='lightcoral'), row=1, col=2)
        
        # MAE
        mae_scores = [results[m]['MAE'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=mae_scores, name='MAE', marker_color='lightgreen'), row=2, col=1)
        
        # CV Scores
        cv_means = [results[m]['CV Score (avg)'] for m in models_list]
        cv_stds = [results[m]['CV Score (std)'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=cv_means, error_y=dict(type='data', array=cv_stds), 
                            name='CV RMSE', marker_color='lightyellow'), row=2, col=2)
        
    else:
        # ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['Accuracy', 'Precision', 'Recall', 'F1-Score'],
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        models_list = list(results.keys())
        
        # Accuracy
        accuracy_scores = [results[m]['Accuracy'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=accuracy_scores, name='Accuracy', marker_color='lightblue'), row=1, col=1)
        
        # Precision
        precision_scores = [results[m]['Precision'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=precision_scores, name='Precision', marker_color='lightcoral'), row=1, col=2)
        
        # Recall
        recall_scores = [results[m]['Recall'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=recall_scores, name='Recall', marker_color='lightgreen'), row=2, col=1)
        
        # F1-Score
        f1_scores = [results[m]['F1-Score'] for m in models_list]
        fig.add_trace(go.Bar(x=models_list, y=f1_scores, name='F1-Score', marker_color='lightyellow'), row=2, col=2)
    
    fig.update_layout(height=600, showlegend=False, title_text="ëª¨ë¸ ì„±ëŠ¥ ì¢…í•© ë¹„êµ")
    st.plotly_chart(fig, use_container_width=True)
    add_chart_export_section(fig, "model_performance_comparison")
    
    # ì˜ˆì¸¡ vs ì‹¤ì œê°’ ë¹„êµ
    st.subheader("ğŸ¯ ì˜ˆì¸¡ vs ì‹¤ì œê°’ ë¹„êµ")
    
    comparison_model = st.selectbox("ë¹„êµí•  ëª¨ë¸ ì„ íƒ", selected_models)
    
    if model_type == "íšŒê·€":
        # ì‚°ì ë„
        fig = go.Figure()
        
        y_pred_comparison = results[comparison_model]['Predictions']
        
        fig.add_trace(go.Scatter(
            x=y_test, y=y_pred_comparison,
            mode='markers',
            name='ì˜ˆì¸¡ê°’',
            marker=dict(color='blue', opacity=0.6)
        ))
        
        # ì™„ë²½í•œ ì˜ˆì¸¡ì„  (y=x)
        min_val = min(y_test.min(), y_pred_comparison.min())
        max_val = max(y_test.max(), y_pred_comparison.max())
        fig.add_trace(go.Scatter(
            x=[min_val, max_val], y=[min_val, max_val],
            mode='lines',
            name='ì™„ë²½í•œ ì˜ˆì¸¡',
            line=dict(color='red', dash='dash')
        ))
        
        fig.update_layout(
            title=f"{comparison_model}: ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’",
            xaxis_title="ì‹¤ì œê°’",
            yaxis_title="ì˜ˆì¸¡ê°’",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        add_chart_export_section(fig, f"prediction_vs_actual_{comparison_model}")
        
    else:
        # í˜¼ë™ í–‰ë ¬
        y_pred_comparison = results[comparison_model]['Predictions']
        cm = confusion_matrix(y_test, y_pred_comparison)
        
        fig = px.imshow(
            cm,
            labels=dict(x="ì˜ˆì¸¡ê°’", y="ì‹¤ì œê°’", color="ê°œìˆ˜"),
            x=['í´ë˜ìŠ¤ 0', 'í´ë˜ìŠ¤ 1'],
            y=['í´ë˜ìŠ¤ 0', 'í´ë˜ìŠ¤ 1'],
            color_continuous_scale='Blues',
            text_auto=True
        )
        fig.update_layout(title=f"{comparison_model}: í˜¼ë™ í–‰ë ¬")
        
        st.plotly_chart(fig, use_container_width=True)
        add_chart_export_section(fig, f"confusion_matrix_{comparison_model}")
        
        # ROC ê³¡ì„  (ì´ì§„ ë¶„ë¥˜ì¸ ê²½ìš°)
        if len(np.unique(y)) == 2:
            st.subheader("ğŸ“ˆ ROC ê³¡ì„ ")
            
            fig = go.Figure()
            
            for model_name in selected_models:
                model = trained_models[model_name]
                
                # ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ëª¨ë¸ë“¤
                if model_name in ['SVM', 'Logistic Regression', 'KNN']:
                    X_test_model = X_test_scaled
                else:
                    X_test_model = X_test
                
                if hasattr(model, "predict_proba"):
                    y_prob = model.predict_proba(X_test_model)[:, 1]
                elif hasattr(model, "decision_function"):
                    y_prob = model.decision_function(X_test_model)
                else:
                    continue
                
                fpr, tpr, _ = roc_curve(y_test, y_prob)
                auc_score = auc(fpr, tpr)
                
                fig.add_trace(go.Scatter(
                    x=fpr, y=tpr,
                    mode='lines',
                    name=f'{model_name} (AUC = {auc_score:.3f})'
                ))
            
            # ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°)
            fig.add_trace(go.Scatter(
                x=[0, 1], y=[0, 1],
                mode='lines',
                name='Random Classifier',
                line=dict(color='gray', dash='dash')
            ))
            
            fig.update_layout(
                title="ROC ê³¡ì„  ë¹„êµ",
                xaxis_title="False Positive Rate",
                yaxis_title="True Positive Rate",
                height=500
            )
            
            st.plotly_chart(fig, use_container_width=True)
            add_chart_export_section(fig, "roc_curves_comparison")
    
    # í•™ìŠµ ê³¡ì„ 
    st.subheader("ğŸ“š í•™ìŠµ ê³¡ì„ ")
    
    learning_model = st.selectbox("í•™ìŠµ ê³¡ì„ ì„ ë³¼ ëª¨ë¸ ì„ íƒ", selected_models, key="learning_curve")
    
    model = models[learning_model]
    
    # ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ëª¨ë¸ë“¤
    if learning_model in ['SVM', 'Logistic Regression', 'KNN']:
        X_learning = X_train_scaled
    else:
        X_learning = X_train
    
    # í•™ìŠµ ê³¡ì„  ê³„ì‚°
    train_sizes, train_scores, val_scores = learning_curve(
        model, X_learning, y_train,
        cv=5, scoring=scoring,
        train_sizes=np.linspace(0.1, 1.0, 10),
        random_state=random_state
    )
    
    # ì ìˆ˜ê°€ ìŒìˆ˜ì¸ ê²½ìš° (MSE) ì–‘ìˆ˜ë¡œ ë³€í™˜
    if scoring == 'neg_mean_squared_error':
        train_scores = -train_scores
        val_scores = -val_scores
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    fig = go.Figure()
    
    # í›ˆë ¨ ì ìˆ˜
    fig.add_trace(go.Scatter(
        x=train_sizes, y=train_mean,
        mode='lines+markers',
        name='Training Score',
        line=dict(color='blue'),
        error_y=dict(type='data', array=train_std, visible=True)
    ))
    
    # ê²€ì¦ ì ìˆ˜
    fig.add_trace(go.Scatter(
        x=train_sizes, y=val_mean,
        mode='lines+markers',
        name='Validation Score',
        line=dict(color='red'),
        error_y=dict(type='data', array=val_std, visible=True)
    ))
    
    score_name = 'Accuracy' if model_type == "ë¶„ë¥˜" else 'RMSE' if scoring == 'neg_mean_squared_error' else 'Score'
    
    fig.update_layout(
        title=f"{learning_model}: í•™ìŠµ ê³¡ì„ ",
        xaxis_title="í›ˆë ¨ ìƒ˜í”Œ ìˆ˜",
        yaxis_title=score_name,
        height=500
    )
    
    st.plotly_chart(fig, use_container_width=True)
    add_chart_export_section(fig, f"learning_curve_{learning_model}")
    
    # ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼
    with st.expander("ğŸ“‹ ëª¨ë¸ë³„ ìƒì„¸ ê²°ê³¼"):
        for model_name in selected_models:
            st.subheader(f"ğŸ” {model_name} ìƒì„¸ ê²°ê³¼")
            
            result = results[model_name]
            
            if model_type == "íšŒê·€":
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("RÂ² Score", f"{result['RÂ²']:.4f}")
                with col2:
                    st.metric("RMSE", f"{result['RMSE']:.4f}")
                with col3:
                    st.metric("MAE", f"{result['MAE']:.4f}")
                with col4:
                    st.metric("CV RMSE", f"{result['CV Score (avg)']:.4f} Â± {result['CV Score (std)']:.4f}")
            else:
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Accuracy", f"{result['Accuracy']:.4f}")
                with col2:
                    st.metric("Precision", f"{result['Precision']:.4f}")
                with col3:
                    st.metric("Recall", f"{result['Recall']:.4f}")
                with col4:
                    st.metric("F1-Score", f"{result['F1-Score']:.4f}")
                
                # ë¶„ë¥˜ ë¦¬í¬íŠ¸
                model = trained_models[model_name]
                if model_name in ['SVM', 'Logistic Regression', 'KNN']:
                    X_test_model = X_test_scaled
                else:
                    X_test_model = X_test
                
                y_pred = model.predict(X_test_model)
                report = classification_report(y_test, y_pred, output_dict=True)
                report_df = pd.DataFrame(report).transpose()
                st.dataframe(report_df.round(4))
            
            st.markdown("---")

if __name__ == "__main__":
    main() 